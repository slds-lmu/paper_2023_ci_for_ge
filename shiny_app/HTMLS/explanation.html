<div style='text-align:left;'><p>

This shiny-app is a visual exploration of a large-scale benchmark experiment that compared various methods for constructing confidence intervals for the generalization error.<br><br>
For a detailed explanation of the experiment setup, see the associated paper.<br><br>

<b>Brief Description of the Experiments</b>:<br>

An inference method is repeatedly applied to a variety of "problems". Such a problem is defined by a tuple (DGP, n, I, L), where the goal is to estimate the generalization error for loss function L and associated CI in the setting of applying inducer I to data consisting of n observations generated from the data generating process DGP.<br>
A problem P has, by design, many concrete instances P, that result from generating specific data of size n using DGP.<br>
These problem instances are then given by tuples (D, I, L).<br>
Each inference method is always applied to 500 concrete problem instance, and from these reptitions we calculate the coverage frequency and average width of the confidence intervals for a specific problem.<br> 
These coverage frequencies can be calculated with respect to the Risk, the Expected Risk, and in some cases a Proxy Quantity (for which the CI method is asymptotically valid).

For an overview of the different DGPs, inference methods, loss functions and inducers, see the "Data" tab, which also includes an explanation of the abbreviations used in the plots.<br>

The results can be found in the same tab.<br><br>

<b>Important Considerations</b>:<br>

<ul>
     <li> Because some inference methods are quite expensive, they are only applied to datasets of size 100 or 500. This explains while the lineplots for some inference methods do not span the whole x-axis.<br>
          Especially the bccv results often appear to be "missing", but this is because they are only applied to dataset of size 100 due to the large amount of required resampling iterations.
     <li> When looking at the aggregated coverage frequency over different problems, it is possible that overcoverage (&gt&gt 95%) for many problems is balanced out by undercoverage (&lt&lt 95%) for other problems. For this reason, we show the root-mean-square coverage error by default.
          It is, however, also possible to look at the coverage frequency aggregated over probles, but this should be done with care. 
     <li> Some CI methods often procude a coverage of 100%. Because overcoverage is capped at 100%, it is important to also look at the width of the confidence intervals.
</ul>

</p><p>&nbsp;</p><hr><p>&nbsp;</p>
</div>
